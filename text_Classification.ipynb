{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjQjLXh1sjyS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import (\n",
        "    Input, Embedding, Conv1D, GRU, LSTM, Dense, Dropout, GlobalMaxPooling1D,\n",
        "    Bidirectional, BatchNormalization, LayerNormalization, SpatialDropout1D,\n",
        "    MultiHeadAttention, Concatenate, Add, GlobalAveragePooling1D\n",
        ")\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwQKNKJFaeBY",
        "outputId": "267ac4e9-836e-4a1f-dad9-e5fafc18a119"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qQ_ixXKDbr-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#preprocessing"
      ],
      "metadata": {
        "id": "s8DHvGp2ak8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load the dataset\n",
        "df_train = pd.read_csv(\"train.csv\")\n",
        "df_test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# Initialize stopwords and lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Preprocessing function for text\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = ' '.join(lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words)\n",
        "    return text\n",
        "\n",
        "# Remove missing values and duplicates\n",
        "df_train = df_train.dropna(subset=['Discussion'])\n",
        "df_test = df_test.dropna(subset=['Discussion'])\n",
        "df_train = df_train.drop_duplicates(subset=['Discussion', 'Category'])\n",
        "\n",
        "# Define category labels\n",
        "category_to_label = {\n",
        "    'Politics': 0,\n",
        "    'Sports': 1,\n",
        "    'Media': 2,\n",
        "    'Market & Economy': 3,\n",
        "    'STEM': 4\n",
        "}\n",
        "df_train['Category'] = df_train['Category'].map(category_to_label)\n",
        "\n",
        "# Apply preprocessing to the text data\n",
        "df_train['Discussion'] = df_train['Discussion'].apply(preprocess_text)\n",
        "df_test['Discussion'] = df_test['Discussion'].apply(preprocess_text)\n",
        "\n",
        "# Remove rows with very short discussions\n",
        "min_length = 5\n",
        "df_train = df_train[df_train['Discussion'].str.len() > min_length]"
      ],
      "metadata": {
        "id": "4EVcvKCEaoVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Tokenize using Keras tokenizer\n",
        "max_len = 100\n",
        "num_words = 20000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=num_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(df_train['Discussion'])\n",
        "word_index = tokenizer.word_index\n",
        "# Save the tokenizer after fitting\n",
        "with open('tokenizer.pkl', 'wb') as file:\n",
        "    pickle.dump(tokenizer, file)\n",
        "# Tokenize and pad the sequences\n",
        "x_train_sequences = tokenizer.texts_to_sequences(df_train['Discussion'])\n",
        "x_train_padded = pad_sequences(x_train_sequences, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "x_test_sequences = tokenizer.texts_to_sequences(df_test['Discussion'])\n",
        "x_test_padded = pad_sequences(x_test_sequences, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "# Encode labels\n",
        "y_train = df_train['Category'].values\n",
        "\n",
        "# Train-validation split\n",
        "x_train_split, x_val_split, y_train_split, y_val_split = train_test_split(\n",
        "    x_train_padded, y_train, test_size=0.2, stratify=y_train, random_state=45\n",
        ")"
      ],
      "metadata": {
        "id": "C_sVB33gbB6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load GloVe embeddings\n",
        "embedding_dim = 200\n",
        "glove_file = 'glove.6B.200d.txt'\n",
        "\n",
        "# Create embedding matrix\n",
        "embeddings_index = {}\n",
        "with open(glove_file, encoding='utf-8') as f:\n",
        "    for line in tqdm(f, desc=\"Loading GloVe embeddings\"):\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        try:\n",
        "            coefs = np.asarray(values[1:], dtype='float32')\n",
        "            # Check if the embedding has the correct dimension\n",
        "            if coefs.shape == (embedding_dim,):\n",
        "                embeddings_index[word] = coefs\n",
        "            else:\n",
        "                print(f\"Skipping word '{word}' due to incorrect embedding dimension: {coefs.shape}\")\n",
        "        except ValueError:\n",
        "            print(f\"Skipping word '{word}' due to value error in embedding\")\n",
        "\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < num_words:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oLzwnfXbGf8",
        "outputId": "195dfc32-d024-4124-9f81-5bc9dbfaaf9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading GloVe embeddings: 34726it [00:01, 19975.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping word 'beltre' due to incorrect embedding dimension: (96,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_predictions_to_csv(predictions, output_directory=r'Predd'):\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "    sample_ids = range(1, len(predictions) + 1)\n",
        "    results_df = pd.DataFrame({'SampleID': sample_ids, 'Category': predictions})\n",
        "    output_file_path = os.path.join(output_directory, 'predictions.csv')\n",
        "    results_df.to_csv(output_file_path, index=False)\n",
        "    print(f\"Predictions saved to: {output_file_path}\")"
      ],
      "metadata": {
        "id": "gJ37ilY8x4Z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "V6xHpjmPejW2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWpcP6bMaLtg"
      },
      "outputs": [],
      "source": [
        "# def train_save_model_and_predictions(model, model_name, output_directory='Output'):\n",
        "#     os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "#     callbacks = [\n",
        "#         EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "#         ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6)\n",
        "#     ]\n",
        "\n",
        "#     # Train the model\n",
        "#     model.fit(\n",
        "#         x_train_split, y_train_split,\n",
        "#         validation_data=(x_val_split, y_val_split),\n",
        "#         epochs=30,\n",
        "#         batch_size=32,\n",
        "#         callbacks=callbacks,\n",
        "#         verbose=1\n",
        "#     )\n",
        "\n",
        "#     # Save the trained model\n",
        "#     model_path = os.path.join(output_directory, f\"{model_name}_model.h5\")\n",
        "#     model.save(model_path)\n",
        "#     print(f\"Model saved to: {model_path}\")\n",
        "\n",
        "#     # Generate predictions\n",
        "#     predictions = model.predict(x_test_padded, verbose=0)\n",
        "#     predictions_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "#     # Define label_to_category mapping (inverse of category_to_label)\n",
        "#     label_to_category = {v: k for k, v in category_to_label.items()}\n",
        "\n",
        "#     # Validate predictions and map to categories\n",
        "#     df_test['Category'] = pd.Series(predictions_labels).map(label_to_category)\n",
        "#     if df_test['Category'].isnull().any():\n",
        "#         print(\"Warning: Some predictions were not mapped to valid categories.\")\n",
        "#         df_test['Category'].fillna('Unknown', inplace=True)\n",
        "\n",
        "#     # Use save_predictions_to_csv function\n",
        "#     save_predictions_to_csv(predictions_labels, output_directory)\n",
        "\n",
        "\n",
        "# def train_save_model_and_predictions(model, model_name, output_directory='Output'):\n",
        "#     # Create a unique subdirectory for the model\n",
        "#     # Create a unique subdirectory for the model\n",
        "#     model_directory = os.path.join(output_directory, model_name)\n",
        "#     os.makedirs(model_directory, exist_ok=True)\n",
        "\n",
        "#     # Callbacks for training\n",
        "#     callbacks = [\n",
        "#         EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "#         ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6),\n",
        "#         ModelCheckpoint(\n",
        "#             # Change the filepath to end with .keras\n",
        "#             filepath=os.path.join(model_directory, f\"{model_name}_best_model.keras\"),\n",
        "#             monitor='val_loss',\n",
        "#             save_best_only=True\n",
        "#         )\n",
        "#     ]\n",
        "\n",
        "#     # Train the model\n",
        "#     model.fit(\n",
        "#         x_train_split, y_train_split,\n",
        "#         validation_data=(x_val_split, y_val_split),\n",
        "#         epochs=30,\n",
        "#         batch_size=32,\n",
        "#         callbacks=callbacks,\n",
        "#         verbose=1\n",
        "#     )\n",
        "\n",
        "#     # Save the final model\n",
        "#     final_model_path = os.path.join(model_directory, f\"{model_name}_final_model.h5\")\n",
        "#     model.save(final_model_path)\n",
        "#     print(f\"Final model saved to: {final_model_path}\")\n",
        "\n",
        "#     # Generate predictions\n",
        "#     predictions = model.predict(x_test_padded, verbose=0)\n",
        "#     predictions_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "#     # Define label_to_category mapping (inverse of category_to_label)\n",
        "#     label_to_category = {v: k for k, v in category_to_label.items()}\n",
        "\n",
        "#     # Validate predictions and map to categories\n",
        "#     df_test['Category'] = pd.Series(predictions_labels).map(label_to_category)\n",
        "#     if df_test['Category'].isnull().any():\n",
        "#         print(\"Warning: Some predictions were not mapped to valid categories.\")\n",
        "#         df_test['Category'].fillna('Unknown', inplace=True)\n",
        "# # Ensure 'Id' column exists, if not, create it\n",
        "#     if 'Id' not in df_test.columns:\n",
        "#         df_test['Id'] = df_test.index + 1  # Assuming Id starts from 1\n",
        "#     # Save predictions to a CSV file\n",
        "#     predictions_path = os.path.join(model_directory, f\"{model_name}_predictions.csv\")\n",
        "#     df_test[['Id', 'Category']].to_csv(predictions_path, index=False)\n",
        "#     print(f\"Predictions saved to: {predictions_path}\")\n",
        "\n",
        "\n",
        "category_to_label = {\n",
        "    'Politics': 0,\n",
        "    'Sports': 1,\n",
        "    'Media': 2,\n",
        "    'Market & Economy': 3,\n",
        "    'STEM': 4\n",
        "}\n",
        "\n",
        "def train_save_model_and_predictions(model, model_name, output_directory='Output'):\n",
        "    # Create a unique subdirectory for the model\n",
        "    model_directory = os.path.join(output_directory, model_name)\n",
        "    os.makedirs(model_directory, exist_ok=True)\n",
        "\n",
        "    # Callbacks for training\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-6),\n",
        "        ModelCheckpoint(\n",
        "            filepath=os.path.join(model_directory, f\"{model_name}_best_model.keras\"),\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(\n",
        "        x_train_split, y_train_split,\n",
        "        validation_data=(x_val_split, y_val_split),\n",
        "        epochs=30,\n",
        "        batch_size=32,\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    # Save the final model\n",
        "    final_model_path = os.path.join(model_directory, f\"{model_name}_final_model.keras\")\n",
        "    model.save(final_model_path)\n",
        "    print(f\"Final model saved to: {final_model_path}\")\n",
        "\n",
        "    # Generate predictions\n",
        "    predictions = model.predict(x_test_padded, verbose=0)\n",
        "    predictions_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "    # Add the encoded predictions to the test DataFrame\n",
        "    df_test['Category'] = predictions_labels\n",
        "    # Ensure 'Id' column exists, if not, create it\n",
        "    if 'Id' not in df_test.columns:\n",
        "        df_test['Id'] = df_test.index + 1  # Assuming Id starts from 1\n",
        "\n",
        "    # Save encoded predictions to a CSV file\n",
        "    predictions_path = os.path.join(model_directory, f\"{model_name}_encoded_predictions.csv\")\n",
        "    df_test[['Id', 'Category']].to_csv(predictions_path, index=False)\n",
        "    print(f\"Encoded predictions saved to: {predictions_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#transformer"
      ],
      "metadata": {
        "id": "hXEDYWRtdQHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transformer_model(vocab_size, embedding_dim, max_len, embedding_matrix):\n",
        "    inputs = Input(shape=(max_len,))\n",
        "    embedding = Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_dim,\n",
        "        weights=[embedding_matrix],\n",
        "        trainable=True\n",
        "    )(inputs)\n",
        "\n",
        "    attention_1 = MultiHeadAttention(\n",
        "        num_heads=8, key_dim=embedding_dim // 8\n",
        "    )(embedding, embedding)\n",
        "    add_1 = Add()([attention_1, embedding])\n",
        "    norm_1 = LayerNormalization()(add_1)\n",
        "\n",
        "    dense_1 = Dense(embedding_dim * 2, activation='relu')(norm_1)\n",
        "    dense_2 = Dense(embedding_dim)(dense_1)\n",
        "    add_2 = Add()([dense_2, norm_1])\n",
        "    norm_2 = LayerNormalization()(add_2)\n",
        "\n",
        "    pooled = GlobalAveragePooling1D()(norm_2)\n",
        "    dropout = Dropout(0.3)(pooled)\n",
        "    outputs = Dense(len(category_to_label), activation='softmax')(dropout)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "ShByEX0qbOc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer_model = build_transformer_model(num_words, embedding_dim, max_len, embedding_matrix)\n",
        "train_save_model_and_predictions(transformer_model, \"transformer_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8okLj4MdPdq",
        "outputId": "38b87c8a-093f-48a8-cbfd-efcf358d98ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 18ms/step - accuracy: 0.5049 - loss: 1.2722 - val_accuracy: 0.6688 - val_loss: 0.8776 - learning_rate: 1.0000e-04\n",
            "Epoch 2/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - accuracy: 0.6616 - loss: 0.8777 - val_accuracy: 0.6850 - val_loss: 0.8239 - learning_rate: 1.0000e-04\n",
            "Epoch 3/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.6980 - loss: 0.8022 - val_accuracy: 0.6871 - val_loss: 0.8170 - learning_rate: 1.0000e-04\n",
            "Epoch 4/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7148 - loss: 0.7623 - val_accuracy: 0.6831 - val_loss: 0.8259 - learning_rate: 1.0000e-04\n",
            "Epoch 5/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7253 - loss: 0.7366 - val_accuracy: 0.6787 - val_loss: 0.8196 - learning_rate: 1.0000e-04\n",
            "Epoch 6/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7392 - loss: 0.7039 - val_accuracy: 0.6909 - val_loss: 0.8151 - learning_rate: 1.0000e-04\n",
            "Epoch 7/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.7445 - loss: 0.6792 - val_accuracy: 0.6897 - val_loss: 0.8248 - learning_rate: 1.0000e-04\n",
            "Epoch 8/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.7591 - loss: 0.6465 - val_accuracy: 0.7065 - val_loss: 0.7895 - learning_rate: 1.0000e-04\n",
            "Epoch 9/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7668 - loss: 0.6297 - val_accuracy: 0.7056 - val_loss: 0.8016 - learning_rate: 1.0000e-04\n",
            "Epoch 10/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7815 - loss: 0.5955 - val_accuracy: 0.7048 - val_loss: 0.7997 - learning_rate: 1.0000e-04\n",
            "Epoch 11/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.7810 - loss: 0.5832 - val_accuracy: 0.7019 - val_loss: 0.8282 - learning_rate: 1.0000e-04\n",
            "Epoch 12/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.8036 - loss: 0.5304 - val_accuracy: 0.6947 - val_loss: 0.8441 - learning_rate: 5.0000e-05\n",
            "Epoch 13/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8130 - loss: 0.5075 - val_accuracy: 0.7027 - val_loss: 0.8324 - learning_rate: 5.0000e-05\n",
            "Final model saved to: Output/transformer_model/transformer_model_final_model.keras\n",
            "Encoded predictions saved to: Output/transformer_model/transformer_model_encoded_predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# model 2"
      ],
      "metadata": {
        "id": "6vEAYDqk3YLg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model definitions\n",
        "def build_enhanced_model(vocab_size, embedding_dim, max_len, embedding_matrix):\n",
        "    model = tf.keras.Sequential([\n",
        "        Embedding(input_dim=vocab_size, output_dim=embedding_dim,\n",
        "                 weights=[embedding_matrix],\n",
        "                 trainable=True),\n",
        "        SpatialDropout1D(0.2),\n",
        "        Conv1D(filters=128, kernel_size=3, padding='same', activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Bidirectional(LSTM(128, return_sequences=True)), # Output a sequence of hidden states\n",
        "        Dropout(0.3),\n",
        "        GlobalMaxPooling1D(),  # Add this line to get a single representation for the sequence\n",
        "        Dense(len(category_to_label), activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-3),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ],
      "metadata": {
        "id": "xEvXrm_LbNax"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enhanced_model = build_enhanced_model(num_words, embedding_dim, max_len, embedding_matrix)\n"
      ],
      "metadata": {
        "id": "bvzAo2BYelqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_save_model_and_predictions(enhanced_model, \"enhanced_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdRzzdZploxb",
        "outputId": "04e597f2-f8fb-446f-a76c-10b6528ff3a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.6023 - loss: 1.0187 - val_accuracy: 0.6882 - val_loss: 0.8283 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 19ms/step - accuracy: 0.7219 - loss: 0.7412 - val_accuracy: 0.7075 - val_loss: 0.7853 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 18ms/step - accuracy: 0.7800 - loss: 0.5965 - val_accuracy: 0.7075 - val_loss: 0.7693 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 18ms/step - accuracy: 0.8246 - loss: 0.4768 - val_accuracy: 0.7063 - val_loss: 0.7917 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 18ms/step - accuracy: 0.8632 - loss: 0.3750 - val_accuracy: 0.6920 - val_loss: 0.8426 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.8904 - loss: 0.3003 - val_accuracy: 0.6930 - val_loss: 0.8973 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.9259 - loss: 0.2090 - val_accuracy: 0.6941 - val_loss: 0.9542 - learning_rate: 5.0000e-04\n",
            "Epoch 8/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 19ms/step - accuracy: 0.9403 - loss: 0.1632 - val_accuracy: 0.6821 - val_loss: 1.0304 - learning_rate: 5.0000e-04\n",
            "Final model saved to: Output/enhanced_model/enhanced_model_final_model.keras\n",
            "Encoded predictions saved to: Output/enhanced_model/enhanced_model_encoded_predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#residual_cnn_model"
      ],
      "metadata": {
        "id": "UR_CAjIcfHAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_residual_cnn_model(vocab_size, embedding_dim, max_len, embedding_matrix):\n",
        "    inputs = Input(shape=(max_len,))\n",
        "    x = Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_dim,\n",
        "        weights=[embedding_matrix],\n",
        "        trainable=True\n",
        "    )(inputs)\n",
        "\n",
        "    proj = Conv1D(128, 1, padding='same')(x)\n",
        "\n",
        "    conv1 = Conv1D(128, 3, padding='same')(x)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv1 = tf.keras.activations.relu(conv1)\n",
        "    conv2 = Conv1D(128, 3, padding='same')(conv1)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "\n",
        "    res1 = Add()([proj, conv2])\n",
        "    res1 = tf.keras.activations.relu(res1)\n",
        "\n",
        "    conv3 = Conv1D(128, 3, padding='same')(res1)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    conv3 = tf.keras.activations.relu(conv3)\n",
        "    conv4 = Conv1D(128, 3, padding='same')(conv3)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "\n",
        "    res2 = Add()([res1, conv4])\n",
        "    res2 = tf.keras.activations.relu(res2)\n",
        "\n",
        "    pooled = GlobalMaxPooling1D()(res2)\n",
        "    dropout = Dropout(0.3)(pooled)\n",
        "    dense = Dense(256, activation='relu')(dropout)\n",
        "    outputs = Dense(len(category_to_label), activation='softmax')(dense)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "-DFUzw10e1FU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "residual_cnn_model = build_residual_cnn_model(num_words, embedding_dim, max_len, embedding_matrix)\n",
        "train_save_model_and_predictions(residual_cnn_model, \"residual_cnn_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hhwtID58e2V3",
        "outputId": "d7384ed1-f4b1-4a28-fe76-91342d5c28a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 17ms/step - accuracy: 0.3380 - loss: 2.1167 - val_accuracy: 0.6165 - val_loss: 0.9749 - learning_rate: 1.0000e-04\n",
            "Epoch 2/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.5751 - loss: 1.1126 - val_accuracy: 0.6533 - val_loss: 0.9005 - learning_rate: 1.0000e-04\n",
            "Epoch 3/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.6378 - loss: 0.9632 - val_accuracy: 0.6665 - val_loss: 0.8741 - learning_rate: 1.0000e-04\n",
            "Epoch 4/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.6714 - loss: 0.8732 - val_accuracy: 0.6663 - val_loss: 0.8569 - learning_rate: 1.0000e-04\n",
            "Epoch 5/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.6921 - loss: 0.8105 - val_accuracy: 0.6817 - val_loss: 0.8283 - learning_rate: 1.0000e-04\n",
            "Epoch 6/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.7192 - loss: 0.7530 - val_accuracy: 0.6850 - val_loss: 0.8315 - learning_rate: 1.0000e-04\n",
            "Epoch 7/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.7477 - loss: 0.6844 - val_accuracy: 0.6836 - val_loss: 0.8385 - learning_rate: 1.0000e-04\n",
            "Epoch 8/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.7678 - loss: 0.6325 - val_accuracy: 0.6884 - val_loss: 0.8378 - learning_rate: 1.0000e-04\n",
            "Epoch 9/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.7917 - loss: 0.5674 - val_accuracy: 0.6953 - val_loss: 0.8531 - learning_rate: 5.0000e-05\n",
            "Epoch 10/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8103 - loss: 0.5206 - val_accuracy: 0.6905 - val_loss: 0.8701 - learning_rate: 5.0000e-05\n",
            "Final model saved to: Output/residual_cnn_model/residual_cnn_model_final_model.keras\n",
            "Encoded predictions saved to: Output/residual_cnn_model/residual_cnn_model_encoded_predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#bidirectional lstm"
      ],
      "metadata": {
        "id": "Pmg7jtDFfOHc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U0sG6M_cvn7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_bilstm_attention_model(vocab_size, embedding_dim, max_len, embedding_matrix):\n",
        "    inputs = Input(shape=(max_len,))\n",
        "    embedding = Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_dim,\n",
        "        weights=[embedding_matrix],\n",
        "        trainable=True\n",
        "    )(inputs)\n",
        "\n",
        "    bilstm = Bidirectional(LSTM(embedding_dim // 2, return_sequences=True))(embedding)\n",
        "    attention = MultiHeadAttention(\n",
        "        num_heads=8, key_dim=embedding_dim // 8\n",
        "    )(bilstm, bilstm)\n",
        "\n",
        "    attention_add = Add()([attention, bilstm])\n",
        "    attention_norm = LayerNormalization()(attention_add)\n",
        "\n",
        "    pooled = GlobalAveragePooling1D()(attention_norm)\n",
        "    dropout = Dropout(0.3)(pooled)\n",
        "    dense = Dense(256, activation='relu')(dropout)\n",
        "    outputs = Dense(len(category_to_label), activation='softmax')(dense)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model"
      ],
      "metadata": {
        "id": "dlHzOZPRe7cG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bilstm_attention_model = build_bilstm_attention_model(num_words, embedding_dim, max_len, embedding_matrix)\n",
        "train_save_model_and_predictions(bilstm_attention_model, \"bilstm_attention_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xifavDErfAaF",
        "outputId": "c495c48b-5e26-4499-882b-8df7d1e6ec76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 23ms/step - accuracy: 0.5248 - loss: 1.1701 - val_accuracy: 0.6575 - val_loss: 0.8908 - learning_rate: 1.0000e-04\n",
            "Epoch 2/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.6827 - loss: 0.8355 - val_accuracy: 0.6789 - val_loss: 0.8476 - learning_rate: 1.0000e-04\n",
            "Epoch 3/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.7085 - loss: 0.7785 - val_accuracy: 0.6979 - val_loss: 0.7940 - learning_rate: 1.0000e-04\n",
            "Epoch 4/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.7231 - loss: 0.7329 - val_accuracy: 0.7056 - val_loss: 0.7881 - learning_rate: 1.0000e-04\n",
            "Epoch 5/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.7420 - loss: 0.6992 - val_accuracy: 0.6941 - val_loss: 0.8140 - learning_rate: 1.0000e-04\n",
            "Epoch 6/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.7494 - loss: 0.6675 - val_accuracy: 0.6945 - val_loss: 0.8107 - learning_rate: 1.0000e-04\n",
            "Epoch 7/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.7664 - loss: 0.6373 - val_accuracy: 0.6901 - val_loss: 0.8499 - learning_rate: 1.0000e-04\n",
            "Epoch 8/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.7853 - loss: 0.5751 - val_accuracy: 0.7048 - val_loss: 0.8242 - learning_rate: 5.0000e-05\n",
            "Epoch 9/30\n",
            "\u001b[1m595/595\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 22ms/step - accuracy: 0.8024 - loss: 0.5461 - val_accuracy: 0.7063 - val_loss: 0.8332 - learning_rate: 5.0000e-05\n",
            "Final model saved to: Output/bilstm_attention_model/bilstm_attention_model_final_model.keras\n",
            "Encoded predictions saved to: Output/bilstm_attention_model/bilstm_attention_model_encoded_predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ensemble model"
      ],
      "metadata": {
        "id": "w679tQQnIFBV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Ensemble prediction function\n",
        "# def ensemble_predict(models, x_test):\n",
        "#     predictions = np.zeros((len(x_test), len(category_to_label)))\n",
        "#     for model in models:\n",
        "#         pred = model.predict(x_test, verbose=0)\n",
        "#         predictions += pred\n",
        "\n",
        "#     predictions /= len(models)\n",
        "#     return np.argmax(predictions, axis=1)"
      ],
      "metadata": {
        "id": "AbeUfRXJOJfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensemble predict function: takes predictions from all models and averages them\n",
        "def ensemble_predict(models, x_test):\n",
        "    # Get predictions from each model\n",
        "    predictions = np.zeros((len(x_test), len(category_to_label)))\n",
        "\n",
        "    for model in models:\n",
        "        model_predictions = model.predict(x_test, verbose=0)\n",
        "        predictions += model_predictions  # Sum predictions\n",
        "\n",
        "    # Average the predictions\n",
        "    predictions /= len(models)\n",
        "\n",
        "    # Return the class with the highest probability (encoded as category)\n",
        "    return np.argmax(predictions, axis=1)"
      ],
      "metadata": {
        "id": "ana4z28qIHpa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensemble models and save predictions function\n",
        "def ensemble_models_and_save_predictions(models, x_test, output_directory='Ensemble_Output'):\n",
        "    # Create output directory\n",
        "    os.makedirs(output_directory, exist_ok=True)\n",
        "\n",
        "    # Get predictions from the ensemble\n",
        "    ensemble_predictions = ensemble_predict(models, x_test)\n",
        "\n",
        "    # Save predictions to CSV\n",
        "    save_predictions_to_csv(ensemble_predictions, output_directory)\n",
        "\n",
        "    print(f\"Ensemble predictions saved to: {output_directory}/predictions.csv\")"
      ],
      "metadata": {
        "id": "eAxtZHP_J0mv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of models\n",
        "models = [transformer_model, enhanced_model, residual_cnn_model, bilstm_attention_model]\n",
        "\n",
        "\n",
        "# Run ensemble model and save predictions\n",
        "ensemble_models_and_save_predictions(models, x_test_padded, output_directory='Ensemble_Output')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPj-lycPIfih",
        "outputId": "091972d6-79d3-4b7d-8454-5b9227bfaa84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to: Ensemble_Output/predictions.csv\n",
            "Ensemble predictions saved to: Ensemble_Output/predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#test script"
      ],
      "metadata": {
        "id": "phIKgEuYTLYd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the test dataset\n",
        "df_test = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# Initialize stopwords and lemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Preprocessing function for text\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    text = ' '.join(lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words)\n",
        "    return text\n",
        "\n",
        "# Remove missing values\n",
        "df_test = df_test.dropna(subset=['Discussion'])\n",
        "\n",
        "# Apply preprocessing to the test data\n",
        "df_test['Discussion'] = df_test['Discussion'].apply(preprocess_text)\n",
        "\n",
        "# Load the pre-trained tokenizer (saved during the training phase)\n",
        "with open('tokenizer.pkl', 'rb') as file:\n",
        "    tokenizer = pickle.load(file)\n",
        "\n",
        "# Tokenize and pad the test data\n",
        "max_len = 100\n",
        "x_test_sequences = tokenizer.texts_to_sequences(df_test['Discussion'])\n",
        "x_test_padded = pad_sequences(x_test_sequences, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "# List of model directories (where models were saved in SavedModel format)\n",
        "model_paths = [\n",
        "    'Output/transformer_model/transformer_model_final_model.keras',\n",
        "    'Output/enhanced_model/enhanced_model_final_model.keras',\n",
        "    'Output/residual_cnn_model/residual_cnn_model_final_model.keras',\n",
        "    'Output/bilstm_attention_model/bilstm_attention_model_final_model.keras'\n",
        "]\n",
        "\n",
        "# Load the models\n",
        "models = []\n",
        "for model_path in model_paths:\n",
        "    model = tf.keras.models.load_model(model_path)  # Load each model from the specified path\n",
        "    models.append(model)\n",
        "\n",
        "# Function to predict using all models\n",
        "def predict_on_test_data(models, x_test_padded):\n",
        "    predictions = []\n",
        "    for model in models:\n",
        "        model_predictions = model.predict(x_test_padded, verbose=0)\n",
        "        predictions.append(model_predictions)\n",
        "\n",
        "    # Average the predictions from all models (for ensemble method)\n",
        "    avg_predictions = np.mean(predictions, axis=0)\n",
        "\n",
        "    # Get the predicted labels (category with the highest probability)\n",
        "    predicted_labels = np.argmax(avg_predictions, axis=1)\n",
        "    return predicted_labels\n",
        "\n",
        "# Generate predictions on the test data\n",
        "predictions = predict_on_test_data(models, x_test_padded)\n",
        "\n",
        "# Create a DataFrame with the results (Id and Category)\n",
        "df_test['Category'] = predictions\n",
        "\n",
        "# Ensure 'Id' column exists, if not, create it\n",
        "if 'Id' not in df_test.columns:\n",
        "    df_test['Id'] = df_test.index + 1  # Assuming Id starts from 1\n",
        "\n",
        "# Save the predictions to a CSV file\n",
        "output_directory = 'tested_Output'  # Folder to save predictions\n",
        "os.makedirs(output_directory, exist_ok=True)\n",
        "predictions_path = os.path.join(output_directory, 'predictions.csv')\n",
        "df_test[['Id', 'Category']].to_csv(predictions_path, index=False)\n",
        "\n",
        "print(f\"Predictions saved to: {predictions_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mkv1JPkjYl-R",
        "outputId": "0fca3ff8-c34d-4a82-b9f4-b016b6f6d1c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions saved to: tested_Output/predictions.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KW378pvxYoeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I0hqbCANYoak"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}